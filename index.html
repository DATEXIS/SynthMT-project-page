<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Synthetic Data Enables Human-Grade Microtubule Analysis by Tuning Segmentation Foundation Models</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <header class="header">
        <div class="container">
            <h1>Synthetic Data Enables Human-Grade Microtubule Analysis by Tuning Segmentation Foundation Models</h1>
            <div class="authors">
                <p class="header-label">Authors</p>
                <p>
                    <a href="mailto:mario.koddenbrock@htw-berlin.de">Mario Koddenbrock</a><sup>*,1</sup>,
                    <a href="mailto:justus.westerhoff@bht-berlin.de">Justus Westerhoff</a><sup>*,2</sup>,
                    <a href="mailto:fachet@mpiib-berlin.mpg.de">Dominik Fachet</a><sup>3</sup>,
                    <a href="mailto:reber@mpiib-berlin.mpg.de">Simone Reber</a><sup>3</sup>,
                    <a href="mailto:felixalexander.gers@bht-berlin.de">Felix Gers</a><sup>2</sup>,
                    <a href="mailto:erik.rodner@htw-berlin.de">Erik Rodner</a><sup>1</sup>
                </p>
                <p class="header-label">Affiliations</p>
                <p class="affiliations">
                    <sup>1</sup><a href="https://kiwerkstatt.f2.htw-berlin.de/">KI Werkstatt, HTW Berlin</a>,
                    <sup>2</sup><a href="https://prof.bht-berlin.de/loeser">DATEXIS, BHT Berlin</a>,
                    <sup>3</sup><a href="https://www.mpiib-berlin.mpg.de/">Max Planck Institute for Infection
                        Biology</a>
                </p>
                <p class="header-label" style="margin-top: 5px;">* These authors contributed equally</p>
            </div>
            <div class="links">
                <a href="#" class="button">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                        style="vertical-align: text-bottom; margin-right: 5px;">
                        <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                        <polyline points="14 2 14 8 20 8"></polyline>
                    </svg>
                    Preprint (arXiv)
                </a>
                <a href="https://github.com/ml-lab-htw/SynthMT" class="button">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                        style="vertical-align: text-bottom; margin-right: 5px;">
                        <path
                            d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
                        </path>
                    </svg>
                    Code (GitHub)
                </a>
                <a href="https://huggingface.co/datasets/HTW-KI-Werkstatt/SynthMT" class="button">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                        style="vertical-align: text-bottom; margin-right: 5px;">
                        <ellipse cx="12" cy="5" rx="9" ry="3"></ellipse>
                        <path d="M21 12c0 1.66-4 3-9 3s-9-1.34-9-3"></path>
                        <path d="M3 5v14c0 1.66 4 3 9 3s9-1.34 9-3V5"></path>
                    </svg>
                    Dataset (Hugging Face)
                </a>
            </div>
        </div>
    </header>

    <section class="interactive-benchmark">
        <div class="container">
            <p style="text-align: center; margin-bottom: 2rem; font-style: italic;">
                We introduce the SynthMT dataset to study the readiness of automated microtubule analysis with
                state-of-the-art foundation models.
            </p>

            <div class="benchmark-interface">
                <!-- Left Column: Image Selector -->
                <div class="column image-selector-column">
                    <h3>Select Image</h3>
                    <div class="framed-box image-selection-box">
                        <div class="image-group synthetic-group">
                            <span class="group-label synthetic-label">SynthMT</span>
                            <div class="image-grid" id="synthetic-grid">
                                <!-- Populated by JS -->
                            </div>
                        </div>
                        <div class="image-group real-group">
                            <span class="group-label real-label">Real</span>
                            <div class="image-grid" id="real-grid">
                                <!-- Populated by JS -->
                            </div>
                        </div>
                    </div>
                    <div class="zoom-hint">üí° Hover over an image for 0.8s to enlarge</div>
                </div>

                <!-- Middle Column: Model Selector -->
                <div class="column model-selector-column">
                    <h3>Select Models</h3>
                    <div class="model-columns-wrapper">
                        <div class="model-list" id="model-list-left">
                            <!-- Baseline + Microscopy models - Populated by JS -->
                        </div>
                        <div class="model-list" id="model-list-right">
                            <!-- General Purpose models - Populated by JS -->
                        </div>
                    </div>
                    <div class="hpo-disclaimer">
                        <strong>HPO</strong> = Hyperparameter Optimization via random search using 10 synthetic images
                        from SynthMT. Each model's parameters were tuned to maximize <a
                            href="https://openaccess.thecvf.com/content_eccv_2018_workshops/w33/html/Liu_Densely_Connected_Stacked_U-network_for_Filament_Segmentation_in_Microscopy_Images_ECCVW_2018_paper.html"
                            target="_blank" title="Skeleton IoU metric for filament segmentation">SKIoU</a>.
                    </div>
                </div>

                <!-- Right Column: Results -->
                <div class="column results-column">
                    <h3>Results</h3>
                    <div class="framed-box results-box">
                        <div class="results-grid" id="results-grid">
                            <div class="placeholder-text">Select an image and models to view results</div>
                        </div>
                    </div>
                    <div class="zoom-hint">üí° Hover over an image for 0.8s to enlarge</div>
                </div>
            </div>
        </div>
    </section>

    <section class="key-contributions">
        <div class="container">
            <h2>Key Contributions</h2>
            <ul>
                <li>üîß <strong>Zero-annotation pipeline:</strong> Got unlabeled MT images? Our open-source pipeline
                    turns them into realistic synthetic training data with perfect ground-truth masks ‚Äî no manual
                    labeling needed!</li>
                <li>üì¶ <strong>Ready-to-use benchmark (<code>SynthMT</code>):</strong> We release <code>SynthMT</code>,
                    a synthetic dataset with instance masks for MTs, judged by domain experts for
                    biological plausibility.</li>
                <li>üìä <strong>Honest model comparison:</strong> We benchmark 9 classical and foundation models on
                    <code>SynthMT</code> ‚Äî zero-shot and with HPO ‚Äî so you know what actually works (spoiler: most
                    don't).
                </li>
                <li>üöÄ <strong>Finally, it works!</strong> <code>SAM3</code> + simple HPO on just 10 synthetic images
                    from our pipeline ‚Üí near-perfect, sometimes <em>superhuman</em> segmentation on real data. Fully
                    automated MT analysis is here! ‚≠ê</li>
            </ul>
        </div>
    </section>

    <section class="benchmark">
        <div class="container">
            <h2>Benchmark Results</h2>

            <div class="benchmark-layout">
                <div class="benchmark-explanation">
                    <h3>üìä What we measure</h3>
                    <p><strong>SKIoU</strong> (Skeleton IoU) measures how well predicted segmentations match
                        ground-truth microtubule shapes ‚Äî the core metric for filament segmentation.</p>
                    <p><strong>Length KL</strong> and <strong>Curvature KL</strong> capture how well the model preserves
                        biologically meaningful properties. Lower = predictions match real MT distributions better.</p>

                    <h3>üî¨ Key takeaways</h3>
                    <ul>
                        <li>Most models struggle out-of-the-box (zero-shot)</li>
                        <li>HPO on just 10 synthetic images helps dramatically</li>
                        <li><code>SAM3Text</code> + HPO achieves <strong>0.89 SKIoU</strong> ‚Äî near-perfect!</li>
                        <li>It also nails downstream metrics (Length KL: 1.01)</li>
                    </ul>

                    <p class="benchmark-note">
                        üí° <em>Full results with all metrics available in the paper.</em>
                    </p>
                </div>

                <div class="benchmark-table-container">
                    <table class="benchmark-table benchmark-table-compact">
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th>SKIoU ‚Üë</th>
                                <th>Length KL ‚Üì</th>
                                <th>Curvature KL ‚Üì</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="group-header">
                                <td colspan="4">Baseline</td>
                            </tr>
                            <tr>
                                <td class="model-name"><code>FIESTA</code></td>
                                <td>0.12</td>
                                <td>5.03</td>
                                <td>1.01</td>
                            </tr>
                            <tr class="hpo-row">
                                <td class="model-name"><code>FIESTA</code> + HPO</td>
                                <td>0.24</td>
                                <td>3.74</td>
                                <td>0.71</td>
                            </tr>

                            <tr class="group-header">
                                <td colspan="4">Microscopy Foundation Models</td>
                            </tr>
                            <tr>
                                <td class="model-name"><code>TARDIS</code></td>
                                <td>0.45</td>
                                <td>0.55</td>
                                <td class="best-score">0.02</td>
                            </tr>
                            <tr>
                                <td class="model-name"><code>CellSAM</code></td>
                                <td>0.49</td>
                                <td>0.19</td>
                                <td class="best-score">0.02</td>
                            </tr>
                            <tr class="hpo-row">
                                <td class="model-name"><code>Cellpose-SAM</code> + HPO</td>
                                <td>0.62</td>
                                <td>0.12</td>
                                <td class="best-score">0.01</td>
                            </tr>

                            <tr class="group-header">
                                <td colspan="4">General Purpose Foundation Models</td>
                            </tr>
                            <tr>
                                <td class="model-name"><code>SAM</code></td>
                                <td>0.37</td>
                                <td>3.90</td>
                                <td>0.73</td>
                            </tr>
                            <tr class="hpo-row">
                                <td class="model-name"><code>SAM2</code> + HPO</td>
                                <td>0.66</td>
                                <td>0.04</td>
                                <td>0.02</td>
                            </tr>
                            <tr>
                                <td class="model-name"><code>SAM3Text</code></td>
                                <td class="best-score">0.80</td>
                                <td>0.07</td>
                                <td>0.06</td>
                            </tr>
                            <tr class="hpo-row winner-row">
                                <td class="model-name"><code>SAM3Text</code> + HPO üèÜ</td>
                                <td class="best-score">0.89</td>
                                <td class="best-score">0.02</td>
                                <td>0.07</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </section>

    <section class="pipeline">
        <div class="container">
            <h2>Synthetic Generation Pipeline</h2>
            <p class="pipeline-subtitle">From structural masks to photorealistic microscopy images in 5 steps</p>

            <div class="pipeline-stages">
                <div class="pipeline-stage">
                    <div class="stage-image">
                        <img src="media/image_pipeline/14_final_mt_mask.png" alt="MT mask generation">
                    </div>
                    <div class="stage-label">
                        <span class="stage-number">1</span>
                        <span class="stage-title">Structural Generation</span>
                    </div>
                    <p class="stage-description">Generate MT masks with realistic geometry</p>
                </div>

                <div class="pipeline-arrow">‚Üí</div>

                <div class="pipeline-stage">
                    <div class="stage-image">
                        <img src="media/image_pipeline/02_after_drawing_mts.png" alt="Physical rendering">
                    </div>
                    <div class="stage-label">
                        <span class="stage-number">2.1</span>
                        <span class="stage-title">Physical Rendering</span>
                    </div>
                    <p class="stage-description">Apply optical system simulation</p>
                </div>

                <div class="pipeline-arrow">‚Üí</div>

                <div class="pipeline-stage">
                    <div class="stage-image">
                        <img src="media/image_pipeline/05_after_random_spots.png" alt="Artifact simulation">
                    </div>
                    <div class="stage-label">
                        <span class="stage-number">2.2</span>
                        <span class="stage-title">Artifact Simulation</span>
                    </div>
                    <p class="stage-description">Add realistic imaging artifacts</p>
                </div>

                <div class="pipeline-arrow">‚Üí</div>

                <div class="pipeline-stage">
                    <div class="stage-image">
                        <img src="media/image_pipeline/09_after_gaussian_noise.png" alt="Noise addition">
                    </div>
                    <div class="stage-label">
                        <span class="stage-number">2.3</span>
                        <span class="stage-title">Noise Addition</span>
                    </div>
                    <p class="stage-description">Inject sensor and shot noise</p>
                </div>

                <div class="pipeline-arrow">‚Üí</div>

                <div class="pipeline-stage">
                    <div class="stage-image">
                        <img src="media/image_pipeline/13_final_frame.png" alt="Final output">
                    </div>
                    <div class="stage-label">
                        <span class="stage-number">2.4</span>
                        <span class="stage-title">Global Distortions</span>
                    </div>
                    <p class="stage-description">Apply intensity and contrast variations</p>
                </div>
            </div>
        </div>
    </section>

    <!-- <section class="dataset">
        <div class="container">
            <h2>SCAM Dataset</h2>
            <p>SCAM is the largest and most diverse real-world typographic attack dataset to date, containing
                images across hundreds of object categories and attack words.</p>

            <div class="dataset-stats">
                <div class="stat-box">
                    <div class="stat">
                        <h4>1,162</h4>
                        <p>Data points</p>
                    </div>
                    <div class="stat">
                        <h4>660</h4>
                        <p>Distinct object labels</p>
                    </div>
                    <div class="stat">
                        <h4>206</h4>
                        <p>Unique attack words</p>
                    </div>
                    <div class="stat">
                        <h4>1,147</h4>
                        <p>Unique object-word combinations</p>
                    </div>
                </div>

                <div class="dataset-comparison">
                    <h4>Attack Word Categories</h4>
                    <div class="comparison-image">
                        <img src="images/category_proportions.svg"
                            alt="Distribution of attack words in SCAM into categories">
                    </div>
                    <p>Distribution of attack words in SCAM into categories, highlighting both everyday terms and
                        safety-critical vocabulary.</p>
                </div>
            </div>
        </div>
    </section>

    <section class="methodology">
        <div class="container">
            <h2>Evaluation Methodology</h2>
            <div class="methodology-container">
                <div class="methodology-vlm">
                    <h3>VLM Evaluation</h3>
                    <img src="images/method_VLM.svg" alt="VLM Evaluation Methodology">
                    <p>We evaluate the performance of VLMs in a zero-shot classification task. For each image, we
                        compute the cosine similarity between its embedding and the text embeddings of both the object
                        label and the attack word. The predicted label is determined based on the highest cosine
                        similarity score.</p>
                </div>
                <div class="methodology-lvlm">
                    <h3>LVLM Evaluation</h3>
                    <img src="images/method_LVLM.svg" alt="LVLM Evaluation Methodology">
                    <p>To assess the robustness of an LVLM against typographic attacks, we evaluate whether its output
                        changes when exposed to typographic modifications. Our evaluation involves providing the model
                        with an image along with a simple prompt asking it to identify what entity is depicted in the
                        image with two options.</p>
                </div>
            </div>
        </div>
    </section>

    <section class="results">
        <div class="container">
            <h2>Results</h2>
            <div class="result-section">
                <h3>Impact on Vision-Language Models</h3>
                <div class="result-figure">
                    <img src="images/all_evals_together.svg" alt="VLM Results" style="max-height: 250px;">
                </div>
                <p>Accuracy distribution of 99 VLMs across NoSCAM, SCAM, and SynthSCAM datasets. VLMs experience an
                    average accuracy drop of 26% when evaluated on the SCAM dataset, with an even steeper decline of 35%
                    on SynthSCAM.</p>
            </div>

            <div class="result-section">
                <h3>Impact on Large Vision-Language Models and Dependence on Prompt</h3>
                <div class="result-figure">
                    <img src="images/lvlm_prompt_evals.svg" alt="LVLM Results" style="max-height: 550px;">
                </div>
                <p>Smaller LLaVA models suffer substantial accuracy drops of 30-50%, while models with larger LLM
                    backbones exhibit better performance under attack.
                    Further, we evaluate whether prompting LVLMs to ignore the attack and focus on the object improves
                    robustness. While we cannot rule out the existence of an effective prompt, our results suggest it is
                    not effective.</p>
            </div>

            <div class="result-section">
                <h3>Impact of Model Parameters</h3>
                <div class="result-figure">
                    <img src="images/model_mparams_vs_lost_to_attack.svg" alt="Model Parameters vs Performance Drop"
                        style="max-height: 250px;">
                </div>
                <p>Susceptibility to typographic attack is agnostic of VLMs size, measured in millions of parameters.
                    While model size alone does not correlate with robustness, larger LLM backbones in LVLMs help
                    mitigate vulnerability to typographic attacks.</p>
            </div>

            <div class="result-section">
                <h3>Impact of Attack Size</h3>
                <div class="result-figure">
                    <img src="images/postit_size_vs_object_win_ratio.svg" alt="Post-it Size vs Object Win Ratio"
                        style="max-height: 300px;">
                </div>
                <p>Model accuracy decreases as the post-it area increases. The size of the attack text correlates with
                    model accuracy, with larger post-it notes causing greater performance degradation.</p>
            </div>
        </div>
    </section>

    <section class="key-findings">
        <div class="container">
            <h2>Key Findings</h2>
            <div class="findings">
                <div class="finding">
                    <div class="icon">üìà</div>
                    <h3>Performance Impact</h3>
                    <p>Typographic attacks cause an average accuracy drop of 26% in VLMs and up to 50% in smaller
                        LLaVA models.</p>
                </div>
                <div class="finding">
                    <div class="icon">üëÅÔ∏è</div>
                    <h3>Vision Encoder Vulnerability</h3>
                    <p>LVLMs inherit vulnerability to typographic attacks from their vision encoders, particularly the
                        ViT-L-14-336 backbone.</p>
                </div>
                <div class="finding">
                    <div class="icon">üß†</div>
                    <h3>LLM Backbone Effect</h3>
                    <p>Larger LLM backbones can compensate for vision encoder limitations, making models more resilient
                        to typographic attacks.</p>
                </div>
                <div class="finding">
                    <div class="icon">üîÑ</div>
                    <h3>Synthetic Validity</h3>
                    <p>Synthetic typographic attacks closely mirror real-world scenarios, validating their use for
                        evaluating model robustness.</p>
                </div>
            </div>
        </div>
    </section> -->

    <section class="abstract">
        <div class="container">
            <h2>Abstract</h2>
            <p>
                Microtubules (MTs) occur in large numbers, and domain experts spend many hours manually segmenting these
                filamentous structures.
                The readiness of current state-of-the-art foundation models for this task cannot be assessed
                systematically because large-scale labeled datasets are missing.
                We address this gap by presenting <code>SynthMT</code>, a synthetic benchmark specifically designed to
                evaluate
                whether current segmentation models are ready for automated MT analysis.
                Our synthetic data generation pipeline produces structurally faithful and labeled images from unlabeled
                real-world data.
                Applied to exemplar IRM frames, this pipeline yields the <code>SynthMT</code> dataset.
                We judge its perceptual realism through expert rating across multiple quality dimensions.
                Using the accompanying evaluation framework, we systematically test nine classical and foundation models
                in both zero-shot and few-shot settings with HPO.
                Across both settings, classical and current foundation models still struggle to achieve the accuracy
                required for downstream biological analysis on, to humans, visually simple <i>in vitro</i> MT IRM
                images.
                However, a notable exception is the recently introduced <code>SAM3</code> model.
                After the HPO with only ten random images from <code>SynthMT</code>, its text-prompt version
                <code>SAM3Text</code> achieves
                near-perfect and in some cases super-human performance on unseen real data.
                This finding shows that fully automated MT segmentation has arrived, but also that such performance
                critically depends on (synthetic) data to guide effective model configuration.
            </p>
        </div>
    </section>

    <section class="resources">
        <div class="container">
            <h2>Resources</h2>
            <div class="resource-links">
                <div class="resource">
                    <div class="icon">
                        <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24" fill="none"
                            stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round">
                            <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                            <polyline points="14 2 14 8 20 8"></polyline>
                            <line x1="16" y1="13" x2="8" y2="13"></line>
                            <line x1="16" y1="17" x2="8" y2="17"></line>
                            <polyline points="10 9 9 9 8 9"></polyline>
                        </svg>
                    </div>
                    <h3>Paper</h3>
                    <a href="#" class="button">arXiv</a>
                </div>
                <div class="resource">
                    <div class="icon">
                        <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24" fill="none"
                            stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round">
                            <path
                                d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
                            </path>
                        </svg>
                    </div>
                    <h3>Code</h3>
                    <a href="https://github.com/ml-lab-htw/SynthMT" class="button">GitHub Repository</a>
                </div>
                <div class="resource">
                    <div class="icon">
                        <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24" fill="none"
                            stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round">
                            <ellipse cx="12" cy="5" rx="9" ry="3"></ellipse>
                            <path d="M21 12c0 1.66-4 3-9 3s-9-1.34-9-3"></path>
                            <path d="M3 5v14c0 1.66 4 3 9 3s9-1.34 9-3V5"></path>
                        </svg>
                    </div>
                    <h3>Dataset</h3>
                    <a href="#" class="button">Hugging Face</a>
                </div>
            </div>
        </div>

    </section>

    <section class="citation">
        <div class="container">
            <h2>Citation</h2>
            <div class="citation-box">
                <pre id="citation-text">@article{koddenbrock2025synthmt,
  title={Synthetic Data Enables Human-Grade Microtubule Analysis by Tuning Segmentation Foundation Models},
  author={Koddenbrock, Mario and Westerhoff, Justus and Fachet, Dominik and Reber, Simone and Gers, Felix and Rodner, Erik},
  journal={arXiv preprint},
  year={2025}
}</pre>
                <button id="copy-citation" class="button">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                        style="vertical-align: text-bottom; margin-right: 5px;">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                    Copy
                </button>
            </div>
        </div>
    </section>

    <section class="logos">
        <div class="container">
            <div class="logo-row">
                <a href="https://www.htw-berlin.de/" target="_blank" rel="noopener">
                    <img src="media/logo_htw.svg" alt="HTW Berlin">
                </a>
                <a href="https://www.bht-berlin.de/" target="_blank" rel="noopener">
                    <img src="media/logo_bht.svg" alt="Berliner Hochschule f√ºr Technik">
                </a>
                <a href="https://www.mpiib-berlin.mpg.de/en" target="_blank" rel="noopener">
                    <img src="media/logo_mpi.svg" alt="Max Planck Institute">
                </a>
            </div>
        </div>
    </section>

    <section class="acknowledgement">
        <div class="container">
            <p class="ack-funding">
                This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) ‚Äì FIP-12 ‚Äì
                Project-ID 528483508
            </p>
            <p class="ack-template">
                We would like to thank the authors of the <a href="https://github.com/Bliss-e-V/SCAM-project-page">SCAM
                    project page</a> for their template.
            </p>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 SynthMT</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>

</html>
