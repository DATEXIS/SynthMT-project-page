<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Synthetic Data Enables Human-Grade Microtubule Analysis by Tuning Segmentation Foundation Models</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <header class="header">
        <div class="container">
            <h1>Synthetic Data Enables Human-Grade Microtubule Analysis by Tuning Segmentation Foundation Models</h1>
            <div class="authors">
                <p class="header-label">Authors</p>
                <p>
                    <a href="mailto:mario.koddenbrock@htw-berlin.de">Mario Koddenbrock</a><sup>*,1</sup>,
                    <a href="mailto:justus.westerhoff@bht-berlin.de">Justus Westerhoff</a><sup>*,2</sup>,
                    <span>Dominik Fachet</span><sup>3</sup>,
                    <span>Simone Reber</span><sup>3</sup>,
                    <span>Felix Gers</span><sup>2</sup>,
                    <a href="mailto:erik.rodner@htw-berlin.de">Erik Rodner</a><sup>1</sup>
                </p>
                <p class="header-label">Affiliations</p>
                <p class="affiliations">
                    <sup>1</sup><a href="https://kiwerkstatt.f2.htw-berlin.de/">KI Werkstatt, HTW Berlin</a>,
                    <sup>2</sup><a href="https://www.bht-berlin.de/">DATEXIS, BHT Berlin</a>,
                    <sup>3</sup><a href="https://www.mpiib-berlin.mpg.de/">Max Planck Institute for Infection
                        Biology</a>
                </p>
                <p class="header-label" style="margin-top: 5px;">* These authors contributed equally</p>
            </div>
            <div class="links">
                <a href="#" class="button">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                        style="vertical-align: text-bottom; margin-right: 5px;">
                        <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                        <polyline points="14 2 14 8 20 8"></polyline>
                    </svg>
                    Preprint (arXiv)
                </a>
                <a href="https://github.com/ml-lab-htw/SynthMT" class="button">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                        style="vertical-align: text-bottom; margin-right: 5px;">
                        <path
                            d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
                        </path>
                    </svg>
                    Code (GitHub)
                </a>
                <a href="https://huggingface.co/datasets/HTW-KI-Werkstatt/SynthMT" class="button">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                        style="vertical-align: text-bottom; margin-right: 5px;">
                        <ellipse cx="12" cy="5" rx="9" ry="3"></ellipse>
                        <path d="M21 12c0 1.66-4 3-9 3s-9-1.34-9-3"></path>
                        <path d="M3 5v14c0 1.66 4 3 9 3s9-1.34 9-3V5"></path>
                    </svg>
                    Dataset (Hugging Face)
                </a>
            </div>
        </div>
    </header>

    <section class="interactive-benchmark">
        <div class="container">
            <p style="text-align: center; margin-bottom: 2rem; font-style: italic;">
                We introduce the SynthMT dataset to study the readiness of automated microtubule analysis with
                state-of-the-art foundation models.
            </p>

            <div class="benchmark-interface">
                <!-- Left Column: Image Selector -->
                <div class="column image-selector-column">
                    <h3>Select Image</h3>
                    <div class="framed-box image-selection-box">
                        <div class="image-group synthetic-group">
                            <span class="group-label synthetic-label">SynthMT</span>
                            <div class="image-grid" id="synthetic-grid">
                                <!-- Populated by JS -->
                            </div>
                        </div>
                        <div class="image-group real-group">
                            <span class="group-label real-label">Real</span>
                            <div class="image-grid" id="real-grid">
                                <!-- Populated by JS -->
                            </div>
                        </div>
                    </div>
                    <div class="zoom-hint">üí° Hover over an image for 0.8s to enlarge</div>
                </div>

                <!-- Middle Column: Model Selector -->
                <div class="column model-selector-column">
                    <h3>Select Models</h3>
                    <div class="model-columns-wrapper">
                        <div class="model-list" id="model-list-left">
                            <!-- Baseline + Microscopy models - Populated by JS -->
                        </div>
                        <div class="model-list" id="model-list-right">
                            <!-- General Purpose models - Populated by JS -->
                        </div>
                    </div>
                    <div class="hpo-disclaimer">
                        <strong>HPO</strong> = Hyperparameter Optimization via random search using 10 synthetic images
                        from SynthMT. Each model's parameters were tuned to maximize <a
                            href="https://openaccess.thecvf.com/content_eccv_2018_workshops/w33/html/Liu_Densely_Connected_Stacked_U-network_for_Filament_Segmentation_in_Microscopy_Images_ECCVW_2018_paper.html"
                            target="_blank" title="Skeleton IoU metric for filament segmentation">SKIoU</a>.
                    </div>
                </div>

                <!-- Right Column: Results -->
                <div class="column results-column">
                    <h3>Results</h3>
                    <div class="framed-box results-box">
                        <div class="results-grid" id="results-grid">
                            <div class="placeholder-text">Select an image and models to view results</div>
                        </div>
                    </div>
                    <div class="zoom-hint">üí° Hover over an image for 0.8s to enlarge</div>
                </div>
            </div>
        </div>
    </section>

    <section class="abstract">
        <div class="container">
            <h2>Abstract</h2>
            <p>
                Microtubules (MTs) occur in large numbers, and domain experts spend many hours manually segmenting these
                filamentous structures.
                The readiness of current state-of-the-art foundation models for this task cannot be assessed
                systematically because large-scale labeled datasets are missing.
                We address this gap by presenting SynthMT, a synthetic benchmark specifically designed to evaluate
                whether current segmentation models are ready for automated MT analysis.
                Our synthetic data generation pipeline produces structurally faithful and labeled images from unlabeled
                real-world data.
                Applied to exemplar IRM frames, this pipeline yields the SynthMT dataset.
                We judge its perceptual realism through expert rating across multiple quality dimensions.
                Using the accompanying evaluation framework, we systematically test nine classical and foundation models
                in both zero-shot and few-shot settings with HPO.
                Across both settings, classical and current foundation models still struggle to achieve the accuracy
                required for downstream biological analysis on, to humans, visually simple <i>in vitro</i> MT IRM
                images.
                However, a notable exception is the recently introduced SAM3 model.
                After the HPO with only ten random images from SynthMT, its text-prompt version SAM3Text achieves
                near-perfect and in some cases super-human performance on unseen real data.
                This finding shows that fully automated MT segmentation has arrived, but also that such performance
                critically depends on (synthetic) data to guide effective model configuration.
            </p>

            <h3>Key Contributions</h3>
            <ul>
                <li><strong>A label-free generation pipeline:</strong> Given only unlabeled real MT images (IRM or TIRF)
                    from a target domain (e.g., specific microscope), our publicly available pipeline adapts to these
                    and generates realistic synthetic images alongside ground-truth masks, enabling adaptation to new
                    labs or conditions without manual annotation.</li>
                <li><strong>Synthetic benchmark dataset (SynthMT):</strong> We apply this pipeline to unlabeled real IRM
                    exemplar frames to construct and release the SynthMT dataset together with instance masks for seeds
                    and MTs. Its biological plausibility is assessed through expert annotation.</li>
                <li><strong>Comprehensive evaluation and tooling:</strong> We perform a reproducible benchmark of
                    classical and state-of-the-art segmentation models on SynthMT, including zero-shot and few-shot
                    adaptation (HPO-based) experiments, establishing quantitative baselines.</li>
                <li><strong>Enabling fully automatic segmentation:</strong> Besides the many limitations that our
                    benchmark reveals, we demonstrate that the newly released foundation model SAM3 reaches
                    near-perfect, in some cases human-grade, performance when guided by simple prompt design and
                    parameter optimization derived solely from our synthetic images. This establishes a practical route
                    toward fully automated MT image segmentation for large-scale experiments.</li>
            </ul>
        </div>
    </section>

    <section class="benchmark">
        <div class="container">
            <h2>Benchmark Results</h2>
            <p>
                Comprehensive benchmark of segmentation models on the SynthMT synthetic dataset signals unprecedented
                performance of the new SAM3 model.
                We report (i) segmentation metrics SKIoU, AP (mean over SKIoU thresholds), and F1 of SKIoU at 0.50 and
                0.75;
                (ii) downstream biological metrics obtained by aggregating all counts, lengths, and curvatures across
                all images and reporting the absolute difference between predicted and ground-truth means ("Diff"),
                together with the KL divergence ("KL") between their distributions (lower is better for all);
                and (iii) computational throughput ("Images/sec."; sequential, unbatched inference on a single NVIDIA
                A100 GPU; FIESTA executed on a MacBook M3 Pro).
                All models are evaluated zero-shot using their default parameters.
                Rows marked with "+ HPO" show performance after HPO with SKIoU on 10 randomly sampled synthetic SynthMT
                images, illustrating few-shot adaptation potential.
                SAM-family models are run in Automatic Instance Segmentation (AIS) mode; SAM3Text uses its text prompt
                mode (default prompt: "thin line").
                The best default and HPO model in every column are listed in <strong>bold</strong>.
            </p>

            <div class="benchmark-table-container">
                <table class="benchmark-table">
                    <thead>
                        <tr>
                            <th rowspan="3">Model</th>
                            <th rowspan="3">Images/sec. &uarr;</th>
                            <th colspan="4">Segmentation</th>
                            <th colspan="5">Downstream</th>
                        </tr>
                        <tr>
                            <th rowspan="2">SKIoU &uarr;</th>
                            <th rowspan="2">AP &uarr;</th>
                            <th colspan="2">F1 &uarr;</th>
                            <th rowspan="2">Count Diff &darr;</th>
                            <th colspan="2">Length &darr;</th>
                            <th colspan="2">Curvature &darr;</th>
                        </tr>
                        <tr>
                            <th>@.50</th>
                            <th>@.75</th>
                            <th>Diff (&mu;m)</th>
                            <th>KL</th>
                            <th>Diff (&mu;m<sup>-1</sup>)</th>
                            <th>KL</th>
                        </tr>
                    </thead>
                    <tbody>
                        <!-- Baseline -->
                        <tr class="group-header">
                            <td colspan="11">Baseline</td>
                        </tr>
                        <tr>
                            <td class="model-name">FIESTA</td>
                            <td>0.21</td>
                            <td>0.12</td>
                            <td>0.12</td>
                            <td>0.22</td>
                            <td>0.18</td>
                            <td>88.35</td>
                            <td>102.86</td>
                            <td>5.03</td>
                            <td>0.19</td>
                            <td>1.01</td>
                        </tr>
                        <tr class="hpo-row">
                            <td class="model-name">&nbsp;&nbsp;+ HPO</td>
                            <td>0.16</td>
                            <td>0.24</td>
                            <td>0.25</td>
                            <td>0.39</td>
                            <td>0.30</td>
                            <td>27.52</td>
                            <td>87.36</td>
                            <td>3.74</td>
                            <td>0.17</td>
                            <td>0.71</td>
                        </tr>

                        <!-- Foundation Models for Microscopy -->
                        <tr class="group-header">
                            <td colspan="11">Foundation Models for Microscopy</td>
                        </tr>
                        <tr>
                            <td class="model-name">StarDist</td>
                            <td>5.04</td>
                            <td>0.01</td>
                            <td>0.03</td>
                            <td>0.00</td>
                            <td>0.00</td>
                            <td>9.44</td>
                            <td>28.85</td>
                            <td>0.69</td>
                            <td>0.08</td>
                            <td>0.19</td>
                        </tr>
                        <tr class="hpo-row">
                            <td class="model-name">&nbsp;&nbsp;+ HPO</td>
                            <td class="best-score">8.85</td>
                            <td>0.30</td>
                            <td>0.34</td>
                            <td>0.35</td>
                            <td>0.15</td>
                            <td>8.60</td>
                            <td>96.22</td>
                            <td>1.94</td>
                            <td>0.07</td>
                            <td>0.19</td>
                        </tr>
                        <tr>
                            <td class="model-name">TARDIS</td>
                            <td>0.32</td>
                            <td>0.45</td>
                            <td>0.48</td>
                            <td>0.59</td>
                            <td>0.45</td>
                            <td>5.85</td>
                            <td>50.73</td>
                            <td>0.55</td>
                            <td>0.02</td>
                            <td class="best-score">0.02</td>
                        </tr>
                        <tr class="hpo-row">
                            <td class="model-name">&nbsp;&nbsp;+ HPO</td>
                            <td>0.31</td>
                            <td>0.48</td>
                            <td>0.55</td>
                            <td>0.57</td>
                            <td>0.41</td>
                            <td>1.04</td>
                            <td>44.97</td>
                            <td>0.41</td>
                            <td class="best-score">0.01</td>
                            <td>0.03</td>
                        </tr>
                        <tr>
                            <td class="model-name">&mu;SAM</td>
                            <td class="best-score">7.41</td>
                            <td>0.01</td>
                            <td>0.06</td>
                            <td>0.01</td>
                            <td>0.00</td>
                            <td>10.99</td>
                            <td>67.41</td>
                            <td>0.96</td>
                            <td>0.03</td>
                            <td>0.16</td>
                        </tr>
                        <tr class="hpo-row">
                            <td class="model-name">&nbsp;&nbsp;+ HPO</td>
                            <td>7.17</td>
                            <td>0.40</td>
                            <td>0.47</td>
                            <td>0.45</td>
                            <td>0.39</td>
                            <td>3.46</td>
                            <td>65.55</td>
                            <td>2.04</td>
                            <td>0.03</td>
                            <td>0.26</td>
                        </tr>
                        <tr>
                            <td class="model-name">CellSAM</td>
                            <td>2.58</td>
                            <td>0.49</td>
                            <td>0.77</td>
                            <td>0.59</td>
                            <td>0.52</td>
                            <td>4.16</td>
                            <td>33.88</td>
                            <td>0.19</td>
                            <td>0.04</td>
                            <td class="best-score">0.02</td>
                        </tr>
                        <tr class="hpo-row">
                            <td class="model-name">&nbsp;&nbsp;+ HPO</td>
                            <td>2.50</td>
                            <td>0.56</td>
                            <td>0.68</td>
                            <td>0.65</td>
                            <td>0.56</td>
                            <td>0.42</td>
                            <td>36.62</td>
                            <td>0.21</td>
                            <td>0.06</td>
                            <td>0.04</td>
                        </tr>
                        <tr>
                            <td class="model-name">Cellpose-SAM</td>
                            <td>2.64</td>
                            <td>0.19</td>
                            <td>0.56</td>
                            <td>0.28</td>
                            <td>0.27</td>
                            <td>9.03</td>
                            <td class="best-score">9.75</td>
                            <td>0.13</td>
                            <td class="best-score">0.00</td>
                            <td class="best-score">0.02</td>
                        </tr>
                        <tr class="hpo-row">
                            <td class="model-name">&nbsp;&nbsp;+ HPO</td>
                            <td>2.59</td>
                            <td>0.62</td>
                            <td>0.74</td>
                            <td>0.73</td>
                            <td>0.62</td>
                            <td>1.07</td>
                            <td>26.94</td>
                            <td>0.12</td>
                            <td class="best-score">0.01</td>
                            <td class="best-score">0.01</td>
                        </tr>

                        <!-- General Purpose Foundation Models -->
                        <tr class="group-header">
                            <td colspan="11">General Purpose Foundation Models</td>
                        </tr>
                        <tr>
                            <td class="model-name">SAM</td>
                            <td>0.72</td>
                            <td>0.37</td>
                            <td>0.46</td>
                            <td>0.45</td>
                            <td>0.44</td>
                            <td>9.18</td>
                            <td>84.93</td>
                            <td>3.90</td>
                            <td>0.07</td>
                            <td>0.73</td>
                        </tr>
                        <tr class="hpo-row">
                            <td class="model-name">&nbsp;&nbsp;+ HPO</td>
                            <td>0.52</td>
                            <td>0.16</td>
                            <td>0.16</td>
                            <td>0.25</td>
                            <td>0.24</td>
                            <td>63.62</td>
                            <td>102.05</td>
                            <td>5.45</td>
                            <td>0.10</td>
                            <td>0.93</td>
                        </tr>
                        <tr>
                            <td class="model-name">SAM2</td>
                            <td>0.97</td>
                            <td>0.01</td>
                            <td>0.04</td>
                            <td>0.01</td>
                            <td>0.01</td>
                            <td>11.23</td>
                            <td>89.53</td>
                            <td>1.77</td>
                            <td>0.18</td>
                            <td>0.28</td>
                        </tr>
                        <tr class="hpo-row">
                            <td class="model-name">&nbsp;&nbsp;+ HPO</td>
                            <td>0.63</td>
                            <td>0.66</td>
                            <td>0.73</td>
                            <td>0.74</td>
                            <td>0.72</td>
                            <td class="best-score">0.39</td>
                            <td>13.11</td>
                            <td>0.04</td>
                            <td>0.03</td>
                            <td>0.02</td>
                        </tr>
                        <tr>
                            <td class="model-name">SAM3</td>
                            <td>1.00</td>
                            <td>0.00</td>
                            <td>0.00</td>
                            <td>0.00</td>
                            <td>0.00</td>
                            <td>11.28</td>
                            <td>93.47</td>
                            <td>2.88</td>
                            <td>0.10</td>
                            <td>0.62</td>
                        </tr>
                        <tr class="hpo-row">
                            <td class="model-name">&nbsp;&nbsp;+ HPO</td>
                            <td>0.79</td>
                            <td>0.28</td>
                            <td>0.29</td>
                            <td>0.33</td>
                            <td>0.33</td>
                            <td>10.96</td>
                            <td>75.60</td>
                            <td>3.84</td>
                            <td class="best-score">0.01</td>
                            <td>0.57</td>
                        </tr>
                        <tr>
                            <td class="model-name">SAM3Text</td>
                            <td>4.24</td>
                            <td class="best-score">0.80</td>
                            <td class="best-score">0.87</td>
                            <td class="best-score">0.87</td>
                            <td class="best-score">0.85</td>
                            <td class="best-score">0.75</td>
                            <td>14.09</td>
                            <td class="best-score">0.07</td>
                            <td>0.10</td>
                            <td>0.06</td>
                        </tr>
                        <tr class="hpo-row">
                            <td class="model-name">&nbsp;&nbsp;+ HPO</td>
                            <td>3.91</td>
                            <td class="best-score">0.89</td>
                            <td class="best-score">0.95</td>
                            <td class="best-score">0.95</td>
                            <td class="best-score">0.93</td>
                            <td>0.49</td>
                            <td class="best-score">1.01</td>
                            <td class="best-score">0.02</td>
                            <td>0.10</td>
                            <td>0.07</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </section>

    <!-- <section class="dataset">
        <div class="container">
            <h2>SCAM Dataset</h2>
            <p>SCAM is the largest and most diverse real-world typographic attack dataset to date, containing
                images across hundreds of object categories and attack words.</p>

            <div class="dataset-stats">
                <div class="stat-box">
                    <div class="stat">
                        <h4>1,162</h4>
                        <p>Data points</p>
                    </div>
                    <div class="stat">
                        <h4>660</h4>
                        <p>Distinct object labels</p>
                    </div>
                    <div class="stat">
                        <h4>206</h4>
                        <p>Unique attack words</p>
                    </div>
                    <div class="stat">
                        <h4>1,147</h4>
                        <p>Unique object-word combinations</p>
                    </div>
                </div>

                <div class="dataset-comparison">
                    <h4>Attack Word Categories</h4>
                    <div class="comparison-image">
                        <img src="images/category_proportions.svg"
                            alt="Distribution of attack words in SCAM into categories">
                    </div>
                    <p>Distribution of attack words in SCAM into categories, highlighting both everyday terms and
                        safety-critical vocabulary.</p>
                </div>
            </div>
        </div>
    </section>

    <section class="methodology">
        <div class="container">
            <h2>Evaluation Methodology</h2>
            <div class="methodology-container">
                <div class="methodology-vlm">
                    <h3>VLM Evaluation</h3>
                    <img src="images/method_VLM.svg" alt="VLM Evaluation Methodology">
                    <p>We evaluate the performance of VLMs in a zero-shot classification task. For each image, we
                        compute the cosine similarity between its embedding and the text embeddings of both the object
                        label and the attack word. The predicted label is determined based on the highest cosine
                        similarity score.</p>
                </div>
                <div class="methodology-lvlm">
                    <h3>LVLM Evaluation</h3>
                    <img src="images/method_LVLM.svg" alt="LVLM Evaluation Methodology">
                    <p>To assess the robustness of an LVLM against typographic attacks, we evaluate whether its output
                        changes when exposed to typographic modifications. Our evaluation involves providing the model
                        with an image along with a simple prompt asking it to identify what entity is depicted in the
                        image with two options.</p>
                </div>
            </div>
        </div>
    </section>

    <section class="results">
        <div class="container">
            <h2>Results</h2>
            <div class="result-section">
                <h3>Impact on Vision-Language Models</h3>
                <div class="result-figure">
                    <img src="images/all_evals_together.svg" alt="VLM Results" style="max-height: 250px;">
                </div>
                <p>Accuracy distribution of 99 VLMs across NoSCAM, SCAM, and SynthSCAM datasets. VLMs experience an
                    average accuracy drop of 26% when evaluated on the SCAM dataset, with an even steeper decline of 35%
                    on SynthSCAM.</p>
            </div>

            <div class="result-section">
                <h3>Impact on Large Vision-Language Models and Dependence on Prompt</h3>
                <div class="result-figure">
                    <img src="images/lvlm_prompt_evals.svg" alt="LVLM Results" style="max-height: 550px;">
                </div>
                <p>Smaller LLaVA models suffer substantial accuracy drops of 30-50%, while models with larger LLM
                    backbones exhibit better performance under attack.
                    Further, we evaluate whether prompting LVLMs to ignore the attack and focus on the object improves
                    robustness. While we cannot rule out the existence of an effective prompt, our results suggest it is
                    not effective.</p>
            </div>

            <div class="result-section">
                <h3>Impact of Model Parameters</h3>
                <div class="result-figure">
                    <img src="images/model_mparams_vs_lost_to_attack.svg" alt="Model Parameters vs Performance Drop"
                        style="max-height: 250px;">
                </div>
                <p>Susceptibility to typographic attack is agnostic of VLMs size, measured in millions of parameters.
                    While model size alone does not correlate with robustness, larger LLM backbones in LVLMs help
                    mitigate vulnerability to typographic attacks.</p>
            </div>

            <div class="result-section">
                <h3>Impact of Attack Size</h3>
                <div class="result-figure">
                    <img src="images/postit_size_vs_object_win_ratio.svg" alt="Post-it Size vs Object Win Ratio"
                        style="max-height: 300px;">
                </div>
                <p>Model accuracy decreases as the post-it area increases. The size of the attack text correlates with
                    model accuracy, with larger post-it notes causing greater performance degradation.</p>
            </div>
        </div>
    </section>

    <section class="key-findings">
        <div class="container">
            <h2>Key Findings</h2>
            <div class="findings">
                <div class="finding">
                    <div class="icon">üìà</div>
                    <h3>Performance Impact</h3>
                    <p>Typographic attacks cause an average accuracy drop of 26% in VLMs and up to 50% in smaller
                        LLaVA models.</p>
                </div>
                <div class="finding">
                    <div class="icon">üëÅÔ∏è</div>
                    <h3>Vision Encoder Vulnerability</h3>
                    <p>LVLMs inherit vulnerability to typographic attacks from their vision encoders, particularly the
                        ViT-L-14-336 backbone.</p>
                </div>
                <div class="finding">
                    <div class="icon">üß†</div>
                    <h3>LLM Backbone Effect</h3>
                    <p>Larger LLM backbones can compensate for vision encoder limitations, making models more resilient
                        to typographic attacks.</p>
                </div>
                <div class="finding">
                    <div class="icon">üîÑ</div>
                    <h3>Synthetic Validity</h3>
                    <p>Synthetic typographic attacks closely mirror real-world scenarios, validating their use for
                        evaluating model robustness.</p>
                </div>
            </div>
        </div>
    </section> -->

    <section class="resources">
        <div class="container">
            <h2>Resources</h2>
            <div class="resource-links">
                <div class="resource">
                    <div class="icon">
                        <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24" fill="none"
                            stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round">
                            <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                            <polyline points="14 2 14 8 20 8"></polyline>
                            <line x1="16" y1="13" x2="8" y2="13"></line>
                            <line x1="16" y1="17" x2="8" y2="17"></line>
                            <polyline points="10 9 9 9 8 9"></polyline>
                        </svg>
                    </div>
                    <h3>Paper</h3>
                    <a href="#" class="button">arXiv</a>
                </div>
                <div class="resource">
                    <div class="icon">
                        <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24" fill="none"
                            stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round">
                            <path
                                d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
                            </path>
                        </svg>
                    </div>
                    <h3>Code</h3>
                    <a href="https://github.com/ml-lab-htw/SynthMT" class="button">GitHub Repository</a>
                </div>
                <div class="resource">
                    <div class="icon">
                        <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24" fill="none"
                            stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round">
                            <ellipse cx="12" cy="5" rx="9" ry="3"></ellipse>
                            <path d="M21 12c0 1.66-4 3-9 3s-9-1.34-9-3"></path>
                            <path d="M3 5v14c0 1.66 4 3 9 3s9-1.34 9-3V5"></path>
                        </svg>
                    </div>
                    <h3>Dataset</h3>
                    <a href="#" class="button">Hugging Face</a>
                </div>
            </div>
        </div>

    </section>

    <section class="citation">
        <div class="container">
            <h2>Citation</h2>
            <div class="citation-box">
                <pre id="citation-text">@article{koddenbrock2025synthmt,
  title={Synthetic Data Enables Human-Grade Microtubule Analysis by Tuning Segmentation Foundation Models},
  author={Koddenbrock, Mario and Westerhoff, Justus and Fachet, Dominik and Reber, Simone and Gers, Felix and Rodner, Erik},
  journal={arXiv preprint},
  year={2025}
}</pre>
                <button id="copy-citation" class="button">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                        style="vertical-align: text-bottom; margin-right: 5px;">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                    Copy
                </button>
            </div>
        </div>
    </section>

    <section class="acknowledgement">
        <div class="container">
            <p style="font-size: 0.9em; color: #666;">
                We would like to thank the authors of the <a href="https://github.com/Bliss-e-V/SCAM-project-page">SCAM
                    project page</a> for their template.
            </p>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 SynthMT</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>

</html>
